本文引用自
>https://spaces.ac.cn/archives/8130
# 绝对位置编码 
将Transformer输入的第$k$个向量$x_k$ 中加入位置向量$p_k$，变为$k_k+p_k$，其中$p_k$只依赖于序列的位置编号$k$.
## 训练式
在BERT和GPT模型中使用的是训练式绝对位置编码，即训练一个矩阵作为，矩阵的每一行就对应一个位置向量.
这种方式的缺点就是没有外推性，即训练和多大的矩阵就只能最多处理多长的序列.
## 三角式
三角式位置编码是论文Attention is all you need提出的:
$$
\begin{cases}
P(k,2i)&=\sin (k/10000^{2i/d})\\
P(k,2i+1)&=\cos(k/10000^{2i/d})
\end{cases}
$$
其中，$P(k,2i)$和$P(k,2i+1)$分别是位置$k$ 的第$2i$和$2i+1$个分量，$d$是向量的维度.
这样的方式具有外推性，只需要将具体的参数代入公式即可求得相应的位置编码.
为了方便起见，作如下变量替换：
$$
w_i=\frac{1}{100000^{2i/d}}
$$
最后位置$k$的位置编码为：
$$
p_k=
\begin{bmatrix}
\sin(w_1k)\\
\cos(w_1k)\\
\sin(w_2k)\\
\cos(w_2k)\\
\vdots\\
\sin(w_{d/2}k)\\
\cos(w_{d/2}k)
\end{bmatrix}
$$
## 三角式表示相对位置
相距位置$k$ $\Delta k$的位置向量为：
$$
\begin{aligned}
P(k+\Delta k,2i)
&=\sin(w_i(k+\Delta k))\\
&=\sin(w_ik)\cos(w_i\Delta k)+\cos(w_ik)\sin(w_i\Delta k)\\
&=\cos(w_i\Delta k)P(k,2i)+\sin(w_i\Delta k)P(k,2i+1)\\

P(k+\Delta k,2i+1)
&=\cos(w_i(k+\Delta k))\\
&=\cos(w_ik)\cos(w_i\Delta k)-\sin(w_ik)\sin(w_i\Delta k)\\
&=\cos(w_i\Delta k)P(k,2i+1)-\sin(w_i\Delta k)P(k,2i)\\
\end{aligned}
$$
其中，$\Delta k$是常数，可以令：
$$
\begin{matrix}
u=\cos(w_i\Delta k)\\
v=\sin(w_i\Delta k)
\end{matrix}
$$
则有：
$$
\begin{bmatrix}
P(k+\Delta k, 2i)\\
P(k+\Delta k, 2k+1)
\end{bmatrix}=
\begin{bmatrix}
u & v\\
-v & u
\end{bmatrix}
\begin{bmatrix}
P(k,2i)\\
P(k,2i+1)
\end{bmatrix}
$$
由此可以看出位置$k$的编码与位置$k+\Delta k$的编码是线性关系。

相距$\Delta k$的两个位置向量的内积为：
$$
\begin{aligned}
p(k)\cdot p(k+\Delta k)&=
\begin{bmatrix}
\sin(w_1k)\\
\cos(w_1k)\\
\sin(w_2k)\\
\cos(w_2k)\\
\vdots\\
\sin(w_{d/2}k)\\
\cos(w_{d/2}k)
\end{bmatrix}
\cdot 
\begin{bmatrix}
\sin(w_1(k+\Delta k))\\
\cos(w_1(k+\Delta k))\\
\sin(w_2(k+\Delta k))\\
\cos(w_2(k+\Delta k))\\
\vdots\\
\sin(w_{d/2}(k+\Delta k))\\
\cos(w_{d/2}(k+\Delta k))
\end{bmatrix}\\
&=\sum \sin(w_i k)\sin(w_i (k+\Delta k))+\cos(w_i k)\cos(w_i(k+\Delta k))\\
&=\sum \cos(w_i(k-(k+\Delta k)))\\
&=\sum \cos(w_i\Delta k)
\end{aligned}
$$
由此可以看出，两个相距$\Delta k$的位置编码的内积只和$\Delta k$有关。