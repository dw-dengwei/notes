# Pretraining Object
## Masked Language Modeling

Masked language modeling (MLM), which was first proposed by Talylor [1953] in the literature, is widely known because the BERT model adapted it as a novel pre-training task. MLM in VLP models is similar to MLM in pre-training language models (PLMs) but predicts the masked textual tokens not only by the rest of the textual tokens but also by the visual tokens. Empirically, VLP models following BERT randomly mask each textual input token with probability 15% and replace the masked one by using a special token [MASK] 80% of the time, a random textual token 10% of the time and the original token 10% of the time to perform masking.

## Prefix Language Modeling

Prefix Language Modeling (PrefixLM) is unified of masked language model and language modeling (LM). PrefixLM is proposed to facilitate the model with solid generation capability that enables text-induced zero-shot generalization without finetuning. PrefixLM differs from the standard LM such that it enables bi-directional attention on the prefix sequence and only conducts autoregressive factorization on the remaining tokens. PrefixLM under the sequence-to-sequence (seq2seq) framework not only enjoys the bidirectional contextualized representation as in MLM but also can perform text generation similar to LM.

## Masked Vision Modeling

Like MLM, masked vision modeling (MVM) samples vision (image or video) regions or patches and usually masks their visual features with a probability of 15%. VLP models need to reconstruct the masked visual features given the remaining visual features and all the textual features. The masked visual features are set to zeros. Because visual features are high-dimensional and continuous, VLP models propose two variants for MVM.

**Masked Features Regression** learns to regress the model output of masked features to its original visual features. VLP models convert the model output of the masked features to a vector of the same dimension as the original visual features first and apply L2 regression between the original visual features and the vector.

**Masked Feature Classification** learns to predict the object semantic class for the masked features. VLP models first feed the output of the masked features into an FC layer to predict the scores of object class, which further goes through a softmax function to be transformed into a prediction normalized distribution. Note that there is no ground-truth label. There are two kinds of methods to train VLP models. One is that VLP models take the most likely object class from the object detection model as the hard label (w.p. 0 or 1), assuming the detected object class is the ground-truth label for the masked features and apply cross-entropy loss to minimize the gap between the prediction and pseudo class. The other is that VLP models utilize soft label as supervision signal, which is the raw output from the detector (i.e., a distribution of object classes) and minimize the KL divergence between two distributions.

## Vision-Language Matching

Vision-Language Matching (VLM) is the most commonly used pre-training objective to align vision and language. In the single-stream VLP models, they use the representation of the special token [CLS] as the fused representation of  both modalities. In the dual-stream VLP models, they concatenate the visual representation of the special visual token [CLSV ] and the textual representation of the special textual token [CLST ] as the fused representation of both modalities. VLP models feed the fused representation of both modalities to an FC layer and a sigmoid function to predict a score between 0 and 1, where 0 indicates the vision and language are mismatched, and 1 indicates the vision and language are matched. During training, VLP models sample positive or negative pairs from the dataset at each step. The negative pair is created by replacing the vision or text in a paired sample with randomly selected from other samples.

## Vision-Language Contrastive Learning

Vision-Language Contrastive Learning (VLC) predicts the matched vision-language pairs from N × N possible visionlanguage pairs given a batch of N vision-language pairs. Note that there are N2 − N negative vision-language pairs within a training batch. VLP models use the visual representation of the special visual token [CLSV ] and the textual representation of the special textual token [CLST ] to denote the aggregated representation of the vision and language, respectively. VLP models compute the softmax-normalized vision (image or video)-to-text similarity and text-to-vision similarity and leverage cross-entropy losses over vision-to-text and text-to-vision similarities to update themselves. The similarity is often implemented by dot products.

## Word-Region Alignment

Word-Region Alignment (WRA) is an unsupervised pre-training objective to align vision regions (vision patches) and words. VLP models utilize Optimal Transport to learn the alignment between vision and language. Empirically, VLP models use the IPOT algorithm to approximate the OT distance since the exact minimization is computationally intractable. After solving minimization, the OT distance serves as the WRA loss to train VLP models.

## Frame Order Modeling

To better model the timing of the video, VLP models randomly disrupt the order of some input frames and then predict the actual position of each frame. Frame Order Modeling (FOM) is modeled as a classification task in practice.

## Particular Pre-training Objects

VLP models also sometimes use the training objects of some downstream tasks, such as visual question answering (VQA) and visual captioning (VC), as pre-training objectives. As for VQA, VLP models take the fused representation mentioned above, apply an FC layer, and use the transformed representation to predict the classification over predefined answer candidates. In addition to VLP models tackling the task as classification over predefined answer candidates, VLP models also can directly generate answers in their original text format. As for VC, to reconstruct the input sentence to endow VLP models with the generation capability, VLP models employ an auto-regressive decoder to generate a corresponding textual description of the image or video.

Note that due to space limitations, we only introduce some popular pre-training objectives. We omit some specific pre-training objectives such as grounding referring expression (GRE), image-conditioned denoising autoencoding (IDA) [Xia et al., 2020], text-conditioned image feature generation (TIFG) [Xia et al., 2020], object detection (OD) [Kamath et al., 2021] and aligned Kaleido patch modeling (AKPM) [Zhuge et al., 2021]. Moreover, we put masked action prediction into the category of MVM.
# Downstream Tasks
## Classification Tasks
**Visual Question Answering (VQA)**. Giving a visual input (image or video), VQA represents the task of correctly providing an answer to a question. It is usually regarded as a classification task where the model predicts the most suitable answer from a pool of choices.

**Visual Reasoning and Compositional Question Answering (GQA)**. GQA is an upgraded version of VQA and aims to advance research on the visual reasoning of natural scenes [Hudson and Manning, 2019]. The images, questions, and answers in its dataset have matching semantic representations. The advantage of this structured representation is that the distribution of answers can be more uniform, and we can analyze the model’s performance from more dimensions.

**Video-Language Inference (VLI)**. Given a video clip with aligned subtitles as a premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip.

**Natural Language for Visual Reasoning (NLVR).** The input of the NLVR task is two images and a text description, and the output is whether the corresponding relationship between the images and the text description is consistent (two labels: true or false).

**Visual Entailment (VE)**. In the VE task, image is the premise, and text is the hypothesis. Our goal is to predict whether the text is “Entailment Image”. There are three labels, Entailment, Neutral, and Contradiction.

**Visual Commonsense Reasoning (VCR)**. VCR exists in the form of multiple-choice questions. For a question, there are several alternative answers. The model must choose an answer from several answers and then select the reason for choosing this answer from several alternative reasons. We can follow VCR’s leaderboard1 to track VLP’s latest ideas.

**Grounding Referring Expressions (GRE)**. The GRE task is to localize an image region given a text reference. The model can output a score for each region, and the region with the highest score is used as the prediction region.

**Category Recognition (CR)**. CR refers to identifying the category of a product which is a vital attribute for describing a product.
## Regression Tasks
**Multi-modal Sentiment Analysis (MSA)**. MSA is aimed to detect sentiments in videos by leveraging multi-modal signals (e.g., vision, language, etc.). It is to predict the affective orientation of an utterance as a continuous intensity variable.
## Retrieval Tasks
**Vision-Language Retrieval (VLR**). VLR involves understanding both vision (image or video) and language domains with appropriate matching strategies. It includes two subtasks, vision-to-text, and text-to-vision retrieval, where vision-to-text retrieval is to fetch the top-most relevant text description from a larger pool of descriptions as per the vision and vice versa.
## Generation Tasks
**Visual Captioning (VC)**. VC aims to generate semantically and syntactically appropriate text descriptions for a given visual (image or video) input.

**Novel Object Captioning at Scale (NoCaps)**. NoCaps extends the VC task to test a model’s capability of describing novel objects from the Open Images dataset, which are unseen in the training corpus [Agrawal et al., 2019].

**Visual Dialogue (VD)**. The task form of VD is given an image (or video), a dialogue history, and a language question, and let the model generate an answer for the question
## Other Tasks
**Multi-modal Machine Translation (MMT)**. MMT is a two-fold task of translation and text generation, translating text from one language to another with additional information from other modalities, i.e., image

**Vision-Language Navigation (VLN)**. VLN is a grounding language task of an agent’s locomotion as it sees and explores the real-world dynamics based on linguistic instructions.

**Optical Character Recognition (OCR)**. OCR generally refers to detecting and recognizing text information in images, which includes two parts: text detection (similar to regression) and text recognition (similar to classification).